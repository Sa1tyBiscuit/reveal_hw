<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport"
			content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">
	
		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet"
			href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section><span style="font-size: 1em;">ðŸ’¬</span>
					<h1 class="r-fit-text">wave chatbot app in four functions</h1>
				</section>
				<section>
					<h3> Development environment Setup</h3>
					<pre><code data-trim data-noescape>
						# Create a new directory
						mkdir wave_chatbot_app
						cd wave_chatbot_app

						# Create a virtual environment
						python3 -m venv venv

						# Activate the virtual environment
						source venv/bin/activate

						# Install the required packages
						pip install h2o-wave h2ogpte
					</code></pre>

				</section>
				<section> On to the the application code! ðŸš€  </section>
				<section>
					<h3>Import the relevant packages</h3>
					<pre><code data-trim data-noescape>
						import os 'testing if this works'
						import asyncio
						
						from h2o_wave import main, app, Q, ui, data, run_on, on
						from h2ogpte import H2OGPTE
						from h2ogpte.types import ChatMessage, PartialChatMessage
					</code></pre>
				</section>
				<section> <h3>1. initialize the Serve Function </h3>
					<pre><code data-trim data-noescape data-line-numbers="2-5|8-17|23-27">
						@app("/")
						async def serve(q: Q):
							"""This serves as the handler function for the chatbot"""
							
							#Setup the application for a new browser tab, 
							
							if not q.client.initialized:  
								q.page["chatbot_card"] = ui.chatbot_card(
									
								#row column width height
									box="1 1 4 -1",
									name="chatbot",
									data=data(
										fields="content from_user",
										t="list",
										rows=[
											[SYSTEM_PROMPT, False],
										],
									),
								)
								q.client.initialized = True
							
							# Route user to the appropriate "on" function
							await run_on(q)
							
							# Update the UI in realtime
							await q.page.save()  
					</code></pre></section>
				<section> <h3>2. Define the Chatbot interaction class</h3></section>
				<section>
					<pre><code data-trim data-noescape data-line-numbers="1-9|11-19">
								class ChatBotInteraction:
								def __init__(self, user_message) -> None:
									self.user_message = user_message
									self.responding = True
							
									self.llm_response = ""
									
									#icon to show when loading response
									self.content_to_show = "ðŸŸ¡"
							
								def update_response(self, message):
									if isinstance(message, ChatMessage):
										self.content_to_show = message.content
										self.responding = False
									
									elif isinstance(message, PartialChatMessage):
										if message.content != "#### LLM Only (no RAG):\n":
											self.llm_response += message.content
											self.content_to_show = self.llm_response + " ðŸŸ¡"

							</code></pre>
				</section>
				<section> <h3>3. Define the Chat function</h3></section>
				<section><pre><code data-trim data-noescape data-line-numbers="1-12 | 14-17 | 19-21 | 23-31 | 33 -35">
					def chat(chatbot_interaction):
						"""
						Send the user's message to the LLM and save the response
						:param chatbot_interaction: Details about the interaction between the user and the LLM
						"""
				
						def stream_response(message):
							"""
							This function is called by the blocking H2OGPTE function periodically
							:param message: response from the LLM, this is either a partial or completed response
							"""
							chatbot_interaction.update_response(message)

						#set environment variables to H2OGPTE_URL and H2OGPTE_API_TOKEN
						client = H2OGPTE(
							address=os.getenv("H2OGPTE_URL"), api_key=os.getenv("H2OGPTE_API_TOKEN")
						)

						#create a temporary collection and chat session
						collection_id = client.create_collection("temp", "")
						chat_session_id = client.create_chat_session(collection_id)

						# send the system prompt and user message to the LLM
						with client.connect(chat_session_id) as session:
							session.query(
								system_prompt=SYSTEM_PROMPT,
								message=chatbot_interaction.user_message,
								timeout=60,
								callback=stream_response,
								rag_config={"rag_type": "llm_only"},
							)

						# delete collection and close chat session
						client.delete_collections([collection_id])
						client.delete_chat_sessions([chat_session_id])
						</code></section>
						<section> <h3>4. Stream updates to UI</h3></section>
						<section><pre><code data-trim data-noescape data-line-numbers="1-9 | 11-18  | 20-25">
							async def stream_updates_to_ui(q: Q):
							"""
							Update the app's UI every 1/10th of a second with values 
							from our chatbot interaction

							:param q: 
							The query object stored by H2O Wave with information 
							about the app and user behavior.
							"""

							# The while loops updates and stream content to the UI
							while q.client.chatbot_interaction.responding:
								q.page["chatbot_card"].data[-1] = [
									q.client.chatbot_interaction.content_to_show,
									False,
								]
								await q.page.save()
								await q.sleep(0.1)

							# Display the response from GPTe to the UI
							q.page["chatbot_card"].data[-1] = [
								q.client.chatbot_interaction.content_to_show,
								False,
							]
							await q.page.save()
						
							</code></pre></section>
							<section> <h3> 5. Finally, the main chatbot function! </h3></section>
						 <section>
							<pre><code  data-trim data-noescape data-line-numbers="5-9 | 11-14">
								@on()
								async def chatbot(q: Q):
									"""Send a user's message to a Large Language Model and stream the response."""

									q.client.chatbot_interaction = ChatBotInteraction(user_message=q.args.chatbot)

									#appends response from the LLM to the chatbot card
									q.page["chatbot_card"].data += [q.args.chatbot, True]
									q.page["chatbot_card"].data += [q.client.chatbot_interaction.content_to_show, False]

									# Prepare our UI-Streaming function so that it can run while the blocking LLM message interaction runs
									update_ui = asyncio.ensure_future(stream_updates_to_ui(q))
									await q.run(chat, q.client.chatbot_interaction)
									await update_ui
							</code></pre>
						</section>
					</div>
				</div>

				<script src="dist/reveal.js"></script>
				<script src="plugin/notes/notes.js"></script>
				<script src="plugin/markdown/markdown.js"></script>
				<script src="plugin/highlight/highlight.js"></script>
				<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
			</body>
		</html>
